[
["index.html", "Data Analytics for Clinical Practice and Research Section 1 Introduction", " Data Analytics for Clinical Practice and Research Gener J Aviles-Rodriguez 2018-07-12 Section 1 Introduction The following examples and implementations have been developed to be presented at the Division of Trauma &amp; Acute Care Surgery of the Loma Linda University Medical Center. Eventhough these implementations are not intended to be an exhaustive review of data analytic tools for the clinical practice, they are presented as broad exmples of trends of implementations that can make significant impact in clinical medicine, research, administration, and health promotion. "],
["clinicalTrials.html", "Section 2 Analysis of Clinical Trials with R 2.1 Why use R in Clinical Trials analysis when there is other software avilable. 2.2 Simulated Clinical Trial", " Section 2 Analysis of Clinical Trials with R 2.1 Why use R in Clinical Trials analysis when there is other software avilable. R is an open source software widely accepted as the lingua franca of statistics. Interestingly it’s use in clinical research has not happened at the same pace as in other disciplines. The field has gravitated mostly to propietary software like SAS or SPSS to name a few. R offers very powerful analytical tools in a modular format (which keep the software as light as possible for the computer performing the analysis) while costing no money to be able to use it (most of the analytical software used in clinical research is proprietary and licences are very expensive). It can also be adjusted to specific needs through the generation of new functions and analytical pipelines. One of the main concerns with clinical trials is the regulatory compliance of the software to FDA rules. The R Foundation for Statistical Computing published a document offeing guidance in this specific topic, the document is updated regularly to stay relevant in the clinical trials environment. 2.2 Simulated Clinical Trial The following is a simulation of a clinical trial to exemplify the use of R in this context. The simulation will be of a simple two-arm clinical trial to compare a new drug to placebo on reducing diastolic blood pressure in hypertensive adult men. We will assume that an appropriate power analysis indicated the sample size required to detect a specified treatment difference is \\(n=100\\) for both treatment groups. For these \\(n\\) participants, we record their age and measure baeline diastolic blood pressure just before randomization. The new drug and placebo are administered and blood pressure is measured and recorded periodically thereafter, including at the end of the trial. Then the change in blood pressure between the endpoint and baselina may be calculated and analyzed as an index of the antihypertensive efficacy of the new drug. 2.2.1 Simulating Data We are assuming that baseline diastolic blood pressures for the 200 recruited participants are normally distributed with a mean \\(\\mu = 100mmHg\\) and a standard deviation \\(\\sigma = 10mmHg\\). Regarding age we are assuming a normally distribution with mean \\(\\mu = 50\\) years, with a standard deviation \\(\\sigma = 10\\) years. We are also assuming that the new drug will decrease diastolic blood pressure by 20 mmHg (\\(\\mu = 20\\)): n = 100 mu = 100 sd = 10 mu.d = 20 age.mu = 50 age.sd = 10 2.2.1.1 Placebo simulations # setting seed for random number generation set.seed(123) age = rnorm(n, age.mu, age.sd) bp.base = rnorm(n,mu,sd) bp.end = rnorm(n,mu,sd) # take the difference between endpoint and baseline bp.diff = bp.end-bp.base # put the data together using &quot;cbind&quot; to column-bind dat4placebo = round(cbind(age,bp.base,bp.end,bp.diff)) head(dat4placebo) ## age bp.base bp.end bp.diff ## [1,] 44 93 122 29 ## [2,] 48 103 113 11 ## [3,] 66 98 97 0 ## [4,] 51 97 105 9 ## [5,] 51 90 96 5 ## [6,] 67 100 95 -4 In the previous table, the results of the first 6 (out of 100 from the placebo subset) simulated participants are shown. 2.2.1.2 New drug simulations age = rnorm(n, age.mu, age.sd) bp.base = rnorm(n,mu,sd) bp.end = rnorm(n,mu-mu.d,sd) bp.diff = bp.end-bp.base dat4drug = round(cbind(age,bp.base,bp.end,bp.diff)) head(dat4drug) ## age bp.base bp.end bp.diff ## [1,] 43 99 74 -25 ## [2,] 42 88 70 -18 ## [3,] 41 94 90 -3 ## [4,] 39 100 88 -12 ## [5,] 46 107 65 -42 ## [6,] 53 83 79 -4 2.2.1.3 Integration of dataset Now to have all results in one structure we will stack the two data sets and add one more variable (column) to the resulting dataset, this new variable will be a nominal value indicating if the patient belongs to the placebo or new drug subgroups. # dataframe holding all data dat = data.frame(rbind(dat4placebo,dat4drug)) # &quot;trt&quot; as a factor for treatment. dat$trt = as.factor(rep(c(&quot;Placebo&quot;, &quot;Drug&quot;), each=n)) head(dat) ## age bp.base bp.end bp.diff trt ## 1 44 93 122 29 Placebo ## 2 48 103 113 11 Placebo ## 3 66 98 97 0 Placebo ## 4 51 97 105 9 Placebo ## 5 51 90 96 5 Placebo ## 6 67 100 95 -4 Placebo 2.2.2 Visualization of data 2.2.2.1 Basic graphs boxplot(dat4placebo, las=1, main=&quot;Placebo&quot;) boxplot(dat4drug, las=1, main=&quot;New Drug&quot;) From these graphs we can see that the data generated is indeed normally distributed. 2.2.2.2 More detailed graphs Before moving to inferential statistical tools, a more detailed visualization is recomended, to undestand a bit more the dynamnics of the data. #load the lattice library library(lattice) # call xyplot function and print it print(xyplot(bp.diff~age|trt, data=dat,xlab=&quot;Age&quot;, strip=strip.custom(bg=&quot;white&quot;), ylab=&quot;Blood Pressure Difference&quot;,lwd=3,cex=1.3,pch=20, type=c(&quot;p&quot;, &quot;r&quot;))) When dealing with blood pressure (BP), there is a reasonable argument to link higher values of BP results with older participants, given the pathophysiology of blood preassure and the stiffening of arteries in older patients. The previous graph clarifies that, in this case, BP results are not being affected by age. Nevertheless, it seems the new drug did lower blood pressur in those who took it. 2.2.3 Inferential statistics After getting acquainted with the data through visualizations, a numerical analysis can be preformed. Staying with a linear approach, we start with a general statistical model as follows: \\[y=\\beta_0 + \\beta_1 \\times trt + \\beta_2 \\times age + \\beta_3age \\times trt + \\epsilon\\] Where: \\(y=\\) change in blood pressure. \\(\\beta_0, \\beta_1, \\beta_2, \\beta_3=\\) parameters. \\(\\epsilon =\\) error, assumed to be independently identically distributed. lm1 = lm(bp.diff~trt*age, data=dat) summary(lm1) ## ## Call: ## lm(formula = bp.diff ~ trt * age, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -37.975 -8.883 0.037 8.488 45.678 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -17.627094 6.242472 -2.824 0.00524 ** ## trtPlacebo 24.153635 9.586926 2.519 0.01255 * ## age -0.076284 0.123209 -0.619 0.53654 ## trtPlacebo:age -0.006311 0.186973 -0.034 0.97311 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 12.8 on 196 degrees of freedom ## Multiple R-squared: 0.4684, Adjusted R-squared: 0.4602 ## F-statistic: 57.56 on 3 and 196 DF, p-value: &lt; 2.2e-16 Making the table easier to the eye: 2.2.3.1 Diagnostics for model assumptions layout(matrix(1:4, nrow=2)) plot(lm1) "],
["interacting-with-the-national-library-of-medicine-.html", "Section 3 Interacting with the National Library of Medicine. 3.1 Searching and datamining the site. 3.2 Visualization", " Section 3 Interacting with the National Library of Medicine. This is an example of an approximation to the database of clinical trials maintained by the National Library of Medicine (NLM) of the National Institutes of Health (NIH). This tool facilitates the exploration and visualization of clinical trials recognized by the government and reported at ClinicalTrials.gov. The rest of the document will show a step by step approximation to the process of datamining and visualization of the information. 3.1 Searching and datamining the site. 3.1.1 Initial subsetting and downloading from website. The following table shows the first 6 studies of a total of found after subsetting accross the whole ClinicalTrials.gov dataset for maches with the terms “acute AND care AND surgery”. Once the selection is located and downloaded, the next step is to extract and keep only the locations in the United States. load(&quot;~/Dropbox/PhD UABC/Colaborations/Loma Linda/bookdown-demo-master/criticalCareClinicTrials.RData&quot;) # First five results of the search (about 1500+ locations): #library(DT) library(DT) #kable(head(c, 5), format = &quot;html&quot;) datatable(head(c, 5)) 3.1.2 Extracting address and getting rid of the rest. # Highest occurence 20 results from the search: library(knitr) test&lt;-e kable(head(test, 10)) address freq 8 Boston,Massachusetts 7 48 New York,New York 7 4 Atlanta,Georgia 6 56 Pittsburgh,Pennsylvania 6 59 Saint Louis,Missouri 6 6 Baltimore,Maryland 5 17 Cleveland,Ohio 4 55 Phoenix,Arizona 4 7 Birmingham,Alabama 3 19 Columbus,Ohio 3 3.2 Visualization Using the Google Maps API to obtain latitud and longitude coordinates from city names and states. 3.2.1 Visualizing results in a map Getting the map background with the right zoom. # load the required libraries library(ggplot2) library(ggmap) # download the map background images map&lt;-get_map(location=&#39;united states&#39;, zoom=4, maptype = &quot;terrain&quot;, source=&#39;google&#39;,color=&#39;color&#39;, force=TRUE) ggmap(map) + theme(axis.line = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), plot.margin = unit(c(0, 0, -1, -1), &#39;lines&#39;)) + xlab(&#39;&#39;) + ylab(&#39;&#39;) 3.2.2 Mapping frequency of trials by city as density. ggmap(map) + geom_point( aes(x=lon, y=lat, show_guide = TRUE, colour=freq.x), data=g, alpha=.5, na.rm = T, size = g$freq.x*0.8) + scale_color_gradient(low=&quot;green&quot;, high=&quot;red&quot;) + theme(axis.line = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), plot.margin = unit(c(0, 0, -1, -1), &#39;lines&#39;)) + xlab(&#39;&#39;) + ylab(&#39;&#39;) 3.2.3 Maping the Heatmap for the same trials. ggmap(map) + geom_density2d(data = h, aes(x = lon, y = lat), size = 0.3)+ stat_density2d(data=h, aes(fill = ..level.., alpha = ..level..), geom=&quot;polygon&quot;, bins=15) + scale_fill_gradient(low = &quot;green&quot;, high = &quot;red&quot;)+ scale_alpha(range = c(0.1, 0.3), guide = FALSE) + theme(axis.line = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), plot.margin = unit(c(0, 0, -1, -1), &#39;lines&#39;)) + xlab(&#39;&#39;) + ylab(&#39;&#39;) 3.2.4 Isolating trials in California #cali &lt;- subset(c,c$address.state==&quot;California&quot;) library(DT) datatable(cali[,1:10]) "],
["machine-learning-for-the-clinical-work.html", "Section 4 Machine Learning for the Clinical Work 4.1 Preprocessing Data 4.2 Model Training", " Section 4 Machine Learning for the Clinical Work Machine learning techniques can be used to analyze medical data and empower clinicians in their decision making processes. Eventhough these models have proven to be very effective in other fields, it is important to keep in mind that the application of machine learning, artificail intelligence and computational statistics to the clinical sciences should always come with close supervision by professionals who understand both approaches. This document shows some applications that could significantly impact the decision making process in the clinical context. 4.1 Preprocessing Data A database from diabetes in the Pima Indians is used for this example: library(healthcareai) str(pima_diabetes) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 768 obs. of 10 variables: ## $ patient_id : int 1 2 3 4 5 6 7 8 9 10 ... ## $ pregnancies : int 6 1 8 1 0 5 3 10 2 8 ... ## $ plasma_glucose: int 148 85 183 89 137 116 78 115 197 125 ... ## $ diastolic_bp : int 72 66 64 66 40 74 50 NA 70 96 ... ## $ skinfold : int 35 29 NA 23 35 NA 32 NA 45 NA ... ## $ insulin : int NA NA NA 94 168 NA 88 NA 543 NA ... ## $ weight_class : chr &quot;obese&quot; &quot;overweight&quot; &quot;normal&quot; &quot;overweight&quot; ... ## $ pedigree : num 0.627 0.351 0.672 0.167 2.288 ... ## $ age : int 50 31 32 21 33 30 26 29 53 54 ... ## $ diabetes : chr &quot;Y&quot; &quot;N&quot; &quot;Y&quot; &quot;N&quot; ... Since the objective here is to find the algorithm that best classifies to predict who will have diabetes and who won´t, a subset of classification algorithms is trained and evaluated to find the best option. load(&quot;~/Dropbox/PhD UABC/Colaborations/Loma Linda/bookdown-demo-master/mlClinical.RData&quot;) #quick_models &lt;- machine_learn(pima_diabetes, patient_id, outcome = diabetes) Once the trainning process is finished, the highest performing algorithm can be selected: quick_models ## Algorithms Trained: Random Forest, k-Nearest Neighbors ## Target: diabetes ## Class: Classification ## Performance Metric: AUROC ## Number of Observations: 768 ## Number of Features: 12 ## Models Trained: 2018-07-12 13:37:01 ## ## Models tuned via 5-fold cross validation over 8 combinations of hyperparameter values. ## Best model: Random Forest ## AUPR = 0.7, AUROC = 0.83 ## Optimal hyperparameter values: ## mtry = 5 ## splitrule = gini ## min.node.size = 15 It is important to highlight the area under a ROC result: 0.85, which can be interpreted as good. This means that we can proceed to a classification process with this model. #predictions &lt;- predict(quick_models) predictions ## # A tibble: 768 x 14 ## diabetes predicted_diabetes pregnancies plasma_glucose diastolic_bp ## * &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Y 0.653 6 148 72 ## 2 N 0.128 1 85 66 ## 3 Y 0.654 8 183 64 ## 4 N 0.00936 1 89 66 ## 5 Y 0.495 0 137 40 ## 6 N 0.257 5 116 74 ## 7 Y 0.0379 3 78 50 ## 8 N 0.577 10 115 72.4 ## 9 Y 0.744 2 197 70 ## 10 Y 0.348 8 125 96 ## # ... with 758 more rows, and 9 more variables: skinfold &lt;dbl&gt;, ## # insulin &lt;dbl&gt;, pedigree &lt;dbl&gt;, age &lt;int&gt;, weight_class_normal &lt;dbl&gt;, ## # weight_class_obese &lt;dbl&gt;, weight_class_overweight &lt;dbl&gt;, ## # weight_class_other &lt;dbl&gt;, weight_class_missing &lt;dbl&gt; plot(predictions) 4.1.1 Data preparation split_data &lt;- split_train_test(d = pima_diabetes, outcome = diabetes, p = .9, seed = 84105) prepped_training_data &lt;- prep_data(split_data$train, patient_id, outcome = diabetes, center = TRUE, scale = TRUE, collapse_rare_factors = FALSE) 4.2 Model Training #models &lt;- tune_models(d = prepped_training_data, # outcome = diabetes, # models = &quot;RF&quot;, # tune_depth = 25, # metric = &quot;PR&quot;) models ## Algorithms Trained: Random Forest ## Target: diabetes ## Class: Classification ## Performance Metric: AUPR ## Number of Observations: 692 ## Number of Features: 13 ## Models Trained: 2018-07-12 13:38:05 ## ## Models tuned via 5-fold cross validation over 23 combinations of hyperparameter values. ## Best model: Random Forest ## AUPR = 0.72, AUROC = 0.84 ## Optimal hyperparameter values: ## mtry = 4 ## splitrule = extratrees ## min.node.size = 13 4.2.1 Clinical importance of each variable get_variable_importance(models) %&gt;% plot() This image is very important for clinical use, because it facilitates the visualization of each variable and how much it imapcts the final result (having diabetes). This analysis can be implemented to different pathologies. "]
]
